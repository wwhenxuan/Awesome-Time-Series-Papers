# Awesome-Time-Series-Papers
ðŸ¥° Some Awesome Time Series Papers for Time Series Analysis.



## Time Series Deep Learning Models

| Title                                                        | Paper                                                  | Code                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------ | ------------------------------------------------------------ |
| Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting | [[paper]](https://arxiv.org/abs/2012.07436)            | [[code]](https://github.com/zhouhaoyi/Informer2020)          |
| Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting | [[paper]](https://arxiv.org/abs/2106.13008)            | [[code]](https://github.com/thuml/Autoformer)                |
| Pyraformer: Low-complexity Pyramidal Attention for Long-range Time Series Modeling and Forecasting | [[paper]](https://openreview.net/pdf?id=0EXmFzUn5I)    | [[code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Pyraformer.py) |
| FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting | [[paper]](https://proceedings.mlr.press/v162/zhou22g)  | [[code]](https://github.com/MAZiqing/FEDformer)              |
| Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting | [[paper]](https://arxiv.org/abs/2205.14415)            | [[code]](https://github.com/thuml/Nonstationary_Transformers) |
| ESTformer: Transformer Utilizing Spatiotemporal Dependencies for Electroencaphalogram Super-resolution | [[paper]](https://arxiv.org/abs/2312.10052)            | [[code]](https://github.com/lucidrains/ETSformer-pytorch)    |
| Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures | [[paper]](https://arxiv.org/abs/2207.01186)            | [[code]](https://github.com/thuml/Time-Series-Library/blob/main/models/Informer.py) |
| FiLM: Frequency improved Legendre Memory Model for Long-term Time Series Forecasting | [[paper]](https://openreview.net/forum?id=zTQdHSQUQWc) | [[code]](https://github.com/tianzhou2011/FiLM/)              |
| Long-term Forecasting with TiDE: Time-series Dense Encoder   | [[paper]](https://arxiv.org/pdf/2304.08424)            | [[code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TiDE.py) |
| A Time Series is Worth 64 Words: Long-term Forecasting with Transformers | [[paper]](https://arxiv.org/abs/2211.14730)            | [[code]](https://github.com/yuqinie98/PatchTST)              |
| Crossformer: Transformer Utilizing Cross-Dimension Dependency for Multivariate Time Series Forecasting | [[paper]](https://openreview.net/forum?id=vSVLM2j9eie) | [[code]](https://github.com/Thinklab-SJTU/Crossformer)       |
| Are Transformers Effective for Time Series Forecasting?      | [[paper]](https://arxiv.org/abs/2205.13504)            | [[code]](https://github.com/cure-lab/LTSF-Linear)            |
| TSMixer: An All-MLP Architecture for Time Series Forecasting | [[paper]](https://arxiv.org/pdf/2303.06053)            | [[code]](https://github.com/thuml/Time-Series-Library/blob/main/models/TSMixer.py) |
| iTransformer: Inverted Transformers Are Effective for Time Series Forecasting | [[paper]](https://arxiv.org/abs/2310.06625)            | [[code]](https://github.com/thuml/iTransformer)              |
| TimeMixer: Decomposable Multiscale Mixing for Time Series Forecasting | [[paper]](http://arxiv.org/abs/2405.14616)             | [[code]](https://github.com/kwuking/TimeMixer)               |
| SAMformer: Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention | [[paper]](https://arxiv.org/abs/2402.10198)            | [[code]](https://github.com/romilbert/samformer)             |
| TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables | [[paper]](https://arxiv.org/abs/2402.19072)            | [[code]](https://github.com/thuml/TimeXer)                   |

## Time Series Multi-Task Model

| Title                                                        | Paper                                       | Code                                                         |
| ------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------ |
| TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis | [[paper]](https://arxiv.org/abs/2210.02186) | [[code]](https://github.com/thuml/Time-Series-Library)       |
| One Fits All:Power General Time Series Analysis by Pretrained LM | [[paper]](https://arxiv.org/abs/2302.11939) | [[code]](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All) |
| Peri-midFormer: Periodic Pyramid Transformer for Time Series Analysis | [[paper]](https://arxiv.org/abs/2411.04554) | [[code]](https://github.com/WuQiangXDU/Peri-midFormer)       |
| UniTS: A Unified Multi-Task Time Series Model                | [[paper]](https://arxiv.org/abs/2403.00131) | [[code]](https://github.com/mims-harvard/UniTS)              |
| Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation | [[paper]](https://arxiv.org/abs/2502.15466) | [[code]]()                                                   |

## Large Language Models for Time Series Analysis

| Title                                                        | Paper                                       | Code                                                         |
| ------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------ |
| One Fits All:Power General Time Series Analysis by Pretrained LM | [[paper]](https://arxiv.org/abs/2302.11939) | [[code]](https://github.com/DAMO-DI-ML/NeurIPS2023-One-Fits-All) |
| Time-LLM: Time Series Forecasting by Reprogramming Large Language Models | [[paper]](https://arxiv.org/abs/2310.01728) | [[code]](https://github.com/KimMeen/Time-LLM)                |
| S2IP-LLM: Semantic Space Informed Prompt Learning with LLM for Time Series Forecasting | [[paper]](https://arxiv.org/abs/2403.05798) | [[code]](https://github.com/panzijie825/s2ip-llm)            |
| A decoder-only foundation model for time-series forecasting  | [[paper]](https://arxiv.org/abs/2310.10688) | [[code]](https://github.com/google-research/timesfm)         |

## Large Time Series Foundation Models

| Title                                                        | Paper                                                 | Code                                                         |
| ------------------------------------------------------------ | ----------------------------------------------------- | ------------------------------------------------------------ |
| TimeGPT-1                                                    | [[paper]](https://arxiv.org/abs/2310.03589)           | [[code]](https://github.com/Nixtla/nixtla)                   |
| Chronos: Learning the Language of Time Series                | [[paper]](https://arxiv.org/abs/2403.07815)           | [[code]](https://github.com/amazon-science/chronos-forecasting) |
| Unified Training of Universal Time Series Forecasting Transformers | [[paper]](https://arxiv.org/abs/2402.02592)           | [[code]](https://github.com/SalesforceAIResearch/uni2ts)     |
| Timer: Generative Pre-trained Transformers Are Large Time Series Models | [[paper]](https://arxiv.org/abs/2402.02368)           | [[code]](https://github.com/thuml/Large-Time-Series-Model)   |
| MOMENT: A Family of Open Time-series Foundation Models       | [[paper]](https://arxiv.org/abs/2402.03885)           | [[code]](https://github.com/moment-timeseries-foundation-model/moment) |
| Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts | [[paper]](https://arxiv.org/abs/2409.16040)           | [[code]](https://github.com/Time-MoE/Time-MoE)               |
| VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters | [[paper]](https://openreview.net/forum?id=IEs29RYxfK) | [[code]](https://github.com/Keytoyze/VisionTS)               |
| Towards Neural Scaling Laws for Time Series Foundation Models | [[paper]](https://arxiv.org/abs/2410.12360)           | [[code]]()                                                   |
| Sundial: A Family of Highly Capable Time Series Foundation Models | [[paper]](https://arxiv.org/abs/2502.00816)           | [[code]]()                                                   |

